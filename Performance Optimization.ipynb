{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance Optimization Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Three algorithms are given in this notebook:\n",
    "    \n",
    "1. Steepest Descent\n",
    "2. Newton's Method \n",
    "3. Conjugate Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Steepest Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to minimise the function $F(\\textbf{x})$ with each iteration. Steepest descent algorithm says that, for the steepest descent to the minimum point of $F(\\textbf{x})$, $\\textbf{x}$ after $k$th iteration is given by \n",
    "\n",
    "$$\\textbf{x}_{k+1} = \\textbf{x}_{k} - \\alpha_{k}\\textbf{g}_k$$\n",
    "\n",
    "where $\\textbf{g}_{k}$ is the gradient of $F$ at $\\textbf{x} = \\textbf{x}_{k}$ and $\\alpha_{k}$ is the learning rate for $k$th iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Rate "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following are a few ways to select the learning rate:\n",
    "\n",
    "1. Select the $\\alpha_{k}$ at each step to minimise $\\textbf{x}_{k+1}$ moving $\\textbf{x}_{k+1}$ along the line $\\textbf{x}_{k} - \\alpha_{k} \\textbf{g}_{k}$.\n",
    "\n",
    "2. Use a constant value like $\\alpha_k = 0.02$ but there are limitations because low values can result in taking a long time to get minimum and large values can result in instability and even increasing values of $F(\\textbf{x})$.\n",
    "\n",
    "3. Use a function of k like $\\alpha_k = \\frac{1}{k}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stable Learning Rates for Quadratic Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us express $F(\\textbf{x})$ as:\n",
    "    \n",
    "$$F(\\textbf{x}) = \\frac{1}{2} \\textbf{x}^T \\textbf{A} \\textbf{x} + \\textbf{d}^T \\textbf{x} + c$$\n",
    "\n",
    "Note that $\\textbf{A}$ is the Hessian matrix.\n",
    "\n",
    "Gradient is given by:\n",
    "\n",
    "$$\\nabla F(\\textbf{x}) = \\textbf{A} \\textbf{x} + \\textbf{d}$$\n",
    "\n",
    "Putting this into expression for steepest descent:\n",
    "\n",
    "$$\\textbf{x}_{k+1} = [\\textbf{I} - \\alpha \\textbf{A}] \\textbf{x}_k - \\alpha \\textbf{d}$$\n",
    "\n",
    "The above linear dynamic system is stable only when eigenvalues of $\\textbf{I} - \\alpha \\textbf{A}$ are less than 1 in magnitude which gives us \n",
    "\n",
    "$$\\alpha < \\frac{2}{\\lambda_{max}}$$\n",
    "\n",
    "where $\\lambda_i$'s are the eigenvalues of the Hessian matrix $\\textbf{A}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minimizing Along a Line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose $\\alpha_k$ to minimize $F(\\textbf{x}_k + \\alpha_{k} \\textbf{p}_k)$.\n",
    "\n",
    "Approximating $F(\\textbf{x})$ upto two terms of taylor series (or if $F(\\textbf{x})$ is quadratic), we can approximate (or precisely say) the best value of $\\alpha_k$ is given by:\n",
    "\n",
    "$$\\alpha_k = - \\frac{\\textbf{g}_k^T \\textbf{p}_k}{\\textbf{p}_k^T \\textbf{A} \\textbf{p}_k}$$\n",
    "\n",
    "where $\\textbf{g}_{k}$ is the gradient of $F$ at $\\textbf{x} = \\textbf{x}_{k}$ and $\\textbf{A}$ is the Hessian matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above algorithms finds the value along the line mentioned earlier for which the function $F(\\textbf{x})$ is minimised. This will happen when learning rate for an iteration is such that line new value of $\\textbf{x}$ is at the intersection of the line and a contour. Next line will be perpendicular to that contour. Hence, the algorithms changes the direction such that it is perpendicular to the direction in previous iteration.\n",
    "\n",
    "Rather than using this, third technique, Conjugate Gradient can be used to minimize function in atmost $n$ steps where n is the dimension of $\\textbf{x}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Newton's Method "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Newton's method is based on second order Taylor series. We use the second order Taylor approximation and assume the function to be that approximation. Minimising that function, we get:\n",
    "\n",
    "$$\\Delta \\textbf{x} = - \\textbf{A}_k^{-1} \\textbf{g}_k$$\n",
    "\n",
    "$$\\textbf{x}_{k+1} = \\textbf{x}_k - \\textbf{A}_k^{-1} \\textbf{g}_k$$\n",
    "\n",
    "This finds the minimum of quadratic function in one step. Otherwise, number of steps depend on inital guess. It has generally faster convergence as compared to steepest descent.\n",
    "\n",
    "But, the method cannot distinguish between local and global minimas since it knows only the local characterstics of the function. Further, it cannot distinguish between minimas, maximas and saddle points. Here, steepest descent is better since its convergence to a local minima is guaranteed. In another notebook, a variation of Newton's method will be implemented in which using steepest descent, whenever divergence will begin to occur, it won't be allowed to diverge.\n",
    "\n",
    "Another problem is the storage of Hessian matrix since it's memory requirement is $O(n^2)$ where n is dimension of $\\textbf{x}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Conjugate Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want quadratic termination like Newton's method and linear memory usage like steepest descent.\n",
    "\n",
    "The change in gradient with iterations is:\n",
    "\n",
    "$$\\Delta \\textbf{g}_k = \\textbf{g}_{k+1} - \\textbf{g}_k = (\\textbf{A} \\textbf{x}_{k+1} + \\textbf{d}) - (\\textbf{A} \\textbf{x}_{k+1} + \\textbf{d}) = \\textbf{A} \\Delta \\textbf{x}_k$$\n",
    "\n",
    "Also, the conjugacy condition can be restated as (for $k \\neq j$):\n",
    "\n",
    "$$\\alpha_k \\textbf{p}_k^T \\textbf{A} \\textbf{p}_j = \\Delta \\textbf{x}^T \\textbf{A} \\textbf{p}_j = \\Delta \\textbf{g}_k^T \\textbf{p}_j = 0$$\n",
    "\n",
    "The search directions will be conjugate if they are perpendicular to change in gradient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the algorithm works as follows:\n",
    "\n",
    "$$\\Delta \\textbf{x} = \\alpha_k \\textbf{p}_k$$\n",
    "\n",
    "$\\alpha_k$ is chosen to minimise along the line $\\textbf{x}_{k+1} = \\textbf{x}_k + \\alpha_k \\textbf{p}_k$\n",
    "\n",
    "$$\\textbf{p}_o = - \\textbf{g}_o$$\n",
    "\n",
    "$$\\textbf{p}_k = - \\textbf{g}_k + \\beta_k \\textbf{p}_{k-1}$$\n",
    "\n",
    "$\\beta_k$ can be given by (any of three):\n",
    "\n",
    "$$\\beta_k = \\frac{\\Delta \\textbf{g}_{k-1}^T \\textbf{g}_k}{\\Delta \\textbf{g}_{k-1}^T \\textbf{p}_{k-1}}$$\n",
    "\n",
    "$$\\beta_k = \\frac{\\textbf{g}_{k}^T \\textbf{g}_k}{\\textbf{g}_{k-1}^T \\textbf{g}_{k-1}}$$\n",
    "\n",
    "$$\\beta_k = \\frac{\\Delta \\textbf{g}_{k-1}^T \\textbf{g}_k}{\\textbf{g}_{k-1}^T \\textbf{g}_{k-1}}$$\n",
    "\n",
    "where $\\textbf{g}_{k}$ is the gradient of $F$ at $\\textbf{x} = \\textbf{x}_{k}$."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
